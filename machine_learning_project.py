# -*- coding: utf-8 -*-
"""Machine Learning Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1o1kVnSqHhkDCJeZwiEjQxprjgxvRAhvi
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.ticker import PercentFormatter

import kagglehub
path = kagglehub.dataset_download("alexteboul/diabetes-health-indicators-dataset")

# reading the csv file
data = pd.read_csv(path + '/diabetes_binary_health_indicators_BRFSS2015.csv')

# copying the original data
df = data.copy()

df

df.columns

df.info()

#Target columns distribution
df['Diabetes_binary'].value_counts(normalize=True)

"""Looks like the dataset is pretty imbalanced, only about 14% of the entries are positive cases. We'll handle that later using an undersampling approach."""

#How many duplicates are there in the entire dataset
n_dups = df.duplicated().sum()
dup_rate = n_dups / len(df)
print(f"Duplicates: {n_dups} rows ({dup_rate:.2%})")

#Distribution of duplicates by target
dup_by_y = df[df.duplicated()]['Diabetes_binary'].value_counts(dropna=False)
print("Duplicates by target:\n", dup_by_y)

"""We decided to keep the duplicates in the dataset, since they do not represent a technical error but rather different individuals with identical characteristics. In medical data, it is common for several patients to share the same features, and removing them could distort the population distribution and reduce the model’s ability to reflect reality. Therefore, keeping the duplicates was intended to preserve the integrity of the original data."""

df.describe()

df.isnull().sum()

#analyze missing data
plt.figure(figsize=(10,5))
sns.heatmap(df.isnull(),cbar=False,cmap='viridis')

#converting age to categorical
age_labels = [
    '18–24', '25–29', '30–34', '35–39', '40–44', '45–49',
    '50–54', '55–59', '60–64', '65–69', '70–74', '75–79', '80+'
]

# Create a new categorical column
df['Age_cat'] = pd.Categorical(
    [age_labels[int(i) - 1] for i in df['Age']],
    categories=age_labels,
    ordered=True
)

edu_labels = [
    'Never attended / Kindergarten',
    'Elementary (Grades 1–8)',
    'Some high school (9–11)',
    'High-school graduate',
    'Some college / technical (1–3 yrs)',
    'College graduate (4+ yrs)'
]

df['Education_cat'] = pd.Categorical(
    [
        edu_labels[int(code) - 1] if pd.notna(code) else np.nan
        for code in df['Education']
    ],
    categories=edu_labels,
    ordered=True
)

income_labels = [
    'Less than $10,000',
    '$10,000–$14,999',
    '$15,000–$19,999',
    '$20,000–$24,999',
    '$25,000–$34,999',
    '$35,000–$49,999',
    '$50,000–$74,999',
    '$75,000 or more'
]

df['Income_cat'] = pd.Categorical(
    [
        income_labels[int(code) - 1] if pd.notna(code) else np.nan  # translate code → label
        for code in df['Income']
    ],
    categories=income_labels,
    ordered=True
)

"""We converted age, education, and income from codes into categorical labels to make the data more interpretable and meaningful for analysis and visualization.

##Data Understanding - EDA

---

### single attribute vs diabetes
"""

fig, axes = plt.subplots(2, 2, figsize=(8, 6))

#Plot 1: Sex vs Diabetes
ax = sns.barplot(x='Sex', y='Diabetes_binary', data=df, ax=axes[0,0])
ax.set_title("Sex vs Diabetes")
ax.set_xlabel("Sex")
ax.set_ylabel("Proportion with Diabetes")
ax.set_xticks([0,1])
ax.set_xticklabels(['Female', 'Male'])

#Plot 2: Fruits vs Diabetes
ax = sns.barplot(x='Fruits', y='Diabetes_binary', data=df, ax=axes[0,1])
ax.set_title("Diabetes vs Fruits Activity")
ax.set_xlabel("Fruits Activity (0 = No, 1 = Yes)")
ax.set_ylabel("Proportion with Diabetes")

#Plot 3: Veggies vs Diabetes
ax = sns.barplot(x='Veggies', y='Diabetes_binary', data=df, ax=axes[1,0])
ax.set_title("Diabetes vs Veggies Activity")
ax.set_xlabel("Veggies Activity (0 = No, 1 = Yes)")
ax.set_ylabel("Proportion with Diabetes")

#Plot 4: HighBP vs Diabetes
ax = sns.barplot(x='HighBP', y='Diabetes_binary', data=df, ax=axes[1,1])
ax.set_title("HighBP vs Diabetes")
ax.set_xlabel("High Blood Pressure (0 = No, 1 = Yes)")
ax.set_ylabel("Proportion with Diabetes")

plt.tight_layout()
plt.show()

"""Men show a slightly higher incidence of diabetes than women, but diet has a greater influence - regular consumption of fruits and vegetables lowers the risk. However, in this section the strongest predictive factor is high blood pressure - which greatly increases the likelihood of diabetes."""

fig, axes = plt.subplots(2, 2, figsize=(8, 6))

#Plot 1: Physical Activity vs Diabetes
ax = sns.barplot(x='PhysActivity', y='Diabetes_binary', data=df, ax=axes[0,0], color="green")
ax.set_title("Diabetes vs Physical Activity")
ax.set_xlabel("Physical Activity (0 = No, 1 = Yes)")
ax.set_ylabel("Proportion with Diabetes")

#Plot 2: High Cholesterol vs Diabetes
ax = sns.barplot(x='HighChol', y='Diabetes_binary', data=df, ax=axes[0,1], color="green")
ax.set_title("Diabetes vs High Cholesterol")
ax.set_xlabel("High Cholesterol (0 = No, 1 = Yes)")
ax.set_ylabel("Proportion with Diabetes")

#Plot 3: Cholesterol Check vs Diabetes
ax = sns.barplot(x='CholCheck', y='Diabetes_binary', data=df, ax=axes[1,0], color="green")
ax.set_title("Diabetes vs Cholesterol Check")
ax.set_xlabel("Cholesterol Check in Last 5 Years (0 = No, 1 = Yes)")
ax.set_ylabel("Proportion with Diabetes")

#Plot 4: Smoking vs Diabetes
ax = sns.barplot(x='Smoker', y='Diabetes_binary', data=df, ax=axes[1,1], color="green")
ax.set_title("Diabetes vs Smoking")
ax.set_xlabel("Smoker (0 = No, 1 = Yes)")
ax.set_ylabel("Proportion with Diabetes")

plt.tight_layout()
plt.show()

"""Individuals who exercise regularly show a much lower prevalence of diabetes, making activity a strong predective factor. High cholesterol is strongly linked to diabetes risk. Those who had a cholesterol check in the past five years also show higher rates - likely because at-risk individuals seek testing. Smoking shows only a slight increase in diabetes prevalence - making it a weaker predictor."""

fig, axes = plt.subplots(2, 2, figsize=(8, 6))

#Plot 1: Stroke vs Diabetes
ax = sns.barplot(x='Stroke', y='Diabetes_binary', data=df, ax=axes[0,0], color="orange")
ax.set_title("Diabetes vs Stroke")
ax.set_xlabel("History of Stroke (0 = No, 1 = Yes)")
ax.set_ylabel("Proportion with Diabetes")

#Plot 2: Heart Disease or Heart Attack vs Diabetes
ax = sns.barplot(x='HeartDiseaseorAttack', y='Diabetes_binary', data=df, ax=axes[0,1], color="orange")
ax.set_title("Diabetes vs Heart Disease/Heart Attack")
ax.set_xlabel("Heart Disease or Heart Attack (0 = No, 1 = Yes)")
ax.set_ylabel("Proportion with Diabetes")

#Plot 3: Difficulty Walking vs Diabetes
ax = sns.barplot(x='DiffWalk', y='Diabetes_binary', data=df, ax=axes[1,0], color="orange")
ax.set_title("Diabetes vs Difficulty Walking")
ax.set_xlabel("Difficulty Walking (0 = No, 1 = Yes)")
ax.set_ylabel("Proportion with Diabetes")

#Plot 4: No Doctor Due to Cost vs Diabetes
ax = sns.barplot(x='NoDocbcCost', y='Diabetes_binary', data=df, ax=axes[1,1], color="orange")
ax.set_title("Diabetes vs No Doctor Visit (Cost Barrier)")
ax.set_xlabel("Could Not See Doctor Due to Cost (0 = No, 1 = Yes)")
ax.set_ylabel("Proportion with Diabetes")

plt.tight_layout()
plt.show()

"""Individuals with a history of stroke or heart disease have a much higher prevalence of diabetes which demonstrate the connection beteween them to diabetes. Difficulty walking is also strongly associated with diabetes. By contrast, not seeing a doctor due to cost shows only a modest increase - making it a weaker predictor."""

fig, axes = plt.subplots(1, 2, figsize=(10, 4))

#Plot 1: Any Healthcare vs Diabetes
ax = sns.barplot(x='AnyHealthcare', y='Diabetes_binary', data=df, ax=axes[0], color="purple")
ax.set_title("Diabetes vs Healthcare Access")
ax.set_xlabel("Any Healthcare Coverage (0 = No, 1 = Yes)")
ax.set_ylabel("Proportion with Diabetes")

#Plot 2: Heavy Alcohol Consumption vs Diabetes
ax = sns.barplot(x='HvyAlcoholConsump', y='Diabetes_binary', data=df, ax=axes[1], color="purple")
ax.set_title("Diabetes vs Heavy Alcohol Consumption")
ax.set_xlabel("Heavy Alcohol Consumption (0 = No, 1 = Yes)")
ax.set_ylabel("Proportion with Diabetes")

plt.tight_layout()
plt.show()

"""Individuals with healthcare coverage show a slightly higher diabetes prevalence - likely because diagnosed patients use healthcare more. In contrast, heavy alcohol consumers report much lower prevalence.

###Demographic analysis
"""

# Use a categorical palette with distinct colors
colors = sns.color_palette("Set2", n_colors=df['Income_cat'].nunique())

plt.figure(figsize=(10, 6))
ax = sns.barplot(
    x='Income_cat',
    y='Diabetes_binary',
    data=df,
    palette=colors,
    errorbar=None
)

ax.set_title("Diabetes Prevalence by Income Level", fontsize=14, fontweight="bold")
ax.set_xlabel("Income Category in $", fontsize=12)
ax.set_ylabel("Prevalence of Diabetes", fontsize=12)

# Format y-axis as percentages
ax.yaxis.set_major_formatter(PercentFormatter(1.0))

plt.xticks(rotation=45, ha="right")

# Add data labels
for c in ax.containers:
    labels = [f"{v.get_height()*100:.1f}%" for v in c]
    ax.bar_label(c, labels=labels, padding=2)

sns.despine()
ax.grid(axis='y', alpha=0.3)

plt.tight_layout()
plt.show()

"""The graph shows that lower income is linked to higher diabetes prevalence - around 25–27% in the lowest brackets, while rates drop below 10% in the highest. This underscores the impact of socioeconomic factors on diabetes risk."""

plt.figure(figsize=(10,6))
sns.countplot(x='Age_cat', data=df, color="lightsteelblue")

plt.title("Distribution of Age Groups", fontsize=14, fontweight="bold")
plt.xlabel("Age Group", fontsize=12)
plt.ylabel("Number of People", fontsize=12)

plt.xticks(rotation=45, ha="right")
plt.grid(axis='y', alpha=0.3)
sns.despine()
plt.tight_layout()
plt.show()

# Count diabetics per age group
counts_age = (
    df.loc[df['Diabetes_binary'] == 1, 'Age_cat']
      .value_counts()
      .reindex(df['Age_cat'].cat.categories)   # keep the correct order
)

plt.figure(figsize=(10,6))
sns.barplot(x=counts_age.index, y=counts_age.values, color="skyblue")

plt.title("Number of People with Diabetes by Age Group", fontsize=14, fontweight="bold")
plt.xlabel("Age Group", fontsize=12)
plt.ylabel("Number of People with Diabetes", fontsize=12)

plt.xticks(rotation=45, ha="right")
plt.grid(axis='y', alpha=0.3)
sns.despine()

plt.tight_layout()
plt.show()

# Use a professional categorical palette with distinct colors
colors = sns.color_palette("Set2", n_colors=df['Age_cat'].nunique())

plt.figure(figsize=(10, 6))
ax = sns.barplot(
    x='Age_cat',
    y='Diabetes_binary',
    data=df,
    palette=colors,
    errorbar=None
)

ax.set_title("Diabetes Prevalence by Age Group", fontsize=14, fontweight="bold")
ax.set_xlabel("Age Group", fontsize=12)
ax.set_ylabel("Prevalence of Diabetes", fontsize=12)

# Format y-axis as %
ax.yaxis.set_major_formatter(PercentFormatter(1.0))

plt.xticks(rotation=45, ha="right")

# Add data labels
for c in ax.containers:
    labels = [f"{v.get_height()*100:.1f}%" for v in c]
    ax.bar_label(c, labels=labels, padding=2)

sns.despine()
ax.grid(axis='y', alpha=0.3)

plt.tight_layout()
plt.show()

"""The graph shows a strong positive relationship between age and diabetes prevalence. In younger age groups (18–34), prevalence is very low, but it rises steadily with age, reaching over 20% among those aged 65 and older. The trend highlights aging as one of the most significant risk factors for diabetes, reflecting both physiological changes and accumulated lifestyle effects over time."""

# Use a categorical palette with distinct colors
colors = sns.color_palette("Set3", n_colors=df['Education_cat'].nunique())

plt.figure(figsize=(10, 6))
ax = sns.barplot(
    x='Education_cat',
    y='Diabetes_binary',
    data=df,
    palette=colors,
    errorbar=None
)

ax.set_title("Diabetes Prevalence by Education Level", fontsize=14, fontweight="bold")
ax.set_xlabel("Education Level", fontsize=12)
ax.set_ylabel("Prevalence of Diabetes", fontsize=12)

# Format y-axis as %
ax.yaxis.set_major_formatter(PercentFormatter(1.0))

plt.xticks(rotation=45, ha="right")

# Add data labels
for c in ax.containers:
    labels = [f"{v.get_height()*100:.1f}%" for v in c]
    ax.bar_label(c, labels=labels, padding=2)

sns.despine()
ax.grid(axis='y', alpha=0.3)

plt.tight_layout()
plt.show()

"""The graph shows an inverse relationship between education and diabetes prevalence: individuals with little or no formal education have the highest rates of diabetes (over 25%), while prevalence decreases steadily as education increases, dropping below 10% among college graduates. This trend suggests that higher education may contribute to better health awareness, lifestyle choices, and access to preventive care, making education an important socioeconomic determinant of diabetes risk.

###BMI analysis
"""

# Define BMI range and bins
bin_min, bin_max = 15, 55
bins = np.arange(bin_min, bin_max + 1)

# Plot BMI distribution (all individuals)
plt.figure(figsize=(10,5))
sns.histplot(df['BMI'], bins=bins, kde=False, color="skyblue")

plt.title("Distribution of BMI in Dataset")
plt.xlabel("BMI")
plt.ylabel("Number of People")

plt.grid(axis='y', alpha=0.3)
sns.despine()
plt.tight_layout()
plt.show()

# Filter BMI values for diabetics only
bmi = df.loc[df['Diabetes_binary'] == 1, 'BMI']

# Define range and bins
bin_min, bin_max = 10, 60
bins = np.arange(bin_min, bin_max + 1, 1)

plt.figure(figsize=(10,5))
sns.histplot(bmi, bins=bins, kde=False, color="teal")

plt.title("Distribution of Diabetic Cases by BMI")
plt.xlabel("BMI")
plt.ylabel("Number of people with diabetes")

plt.grid(axis='y', alpha=0.3)
sns.despine()

plt.tight_layout()
plt.show()

"""This graph is misleading because it shows where most diabetics fall in absolute numbers, not the actual risk of diabetes at each BMI level."""

# Round BMI for binning
df['BMI_rounded'] = df['BMI'].round().astype(int)

# Define bins
bin_min, bin_max = 10, 60
bins = np.arange(bin_min, bin_max + 1)

# Calculate prevalence (%) in each bin
prevalence = []
for b in bins:
    group = df[df['BMI_rounded'] == b]
    if len(group) > 0:
        prevalence.append(100 * group['Diabetes_binary'].mean())
    else:
        prevalence.append(0)

plt.figure(figsize=(10,5))
plt.bar(bins, prevalence, width=0.9, color="teal", alpha=0.8)

plt.title("Prevalence of Diabetes by BMI")
plt.xlabel("BMI (rounded)")
plt.ylabel("Diabetes Prevalence (%)")

plt.grid(axis='y', alpha=0.3)
sns.despine()
plt.tight_layout()
plt.show()

# Clinical BMI bins
bmi_bins   = [0, 18.5, 25, 30, 35, 40, float('inf')]
bmi_labels = ['Underweight','Normal','Overweight','Obese I','Obese II','Obese III']

df['BMI_cat'] = pd.Categorical(
    pd.cut(df['BMI'], bins=bmi_bins, labels=bmi_labels, right=False),
    categories=bmi_labels, ordered=True
)

# Traffic-light palette
color_map = {
    'Underweight': '#87CEEB',  # neutral light blue
    'Normal':      '#2ECC71',  # green
    'Overweight':  '#F1C40F',  # yellow
    'Obese I':     '#E67E22',  # orange
    'Obese II':    '#E74C3C',  # red
    'Obese III':   '#C0392B'   # dark red
}

plt.figure(figsize=(10, 6))
ax = sns.barplot(
    x='BMI_cat',
    y='Diabetes_binary',
    hue='BMI_cat',
    data=df,
    palette=color_map,
    dodge=False,
    errorbar=None
)

ax.set_title("Diabetes Prevalence by BMI Category", fontsize=14, fontweight="bold")
ax.set_xlabel("BMI Category", fontsize=12)
ax.set_ylabel("Prevalence of Diabetes", fontsize=12)

# Format y-axis as %
ax.yaxis.set_major_formatter(PercentFormatter(1.0))

for c in ax.containers:
    labels = [f"{v.get_height()*100:.0f}%" for v in c]
    ax.bar_label(c, labels=labels, padding=2)

sns.despine()
ax.grid(axis='y', alpha=0.3)
ax.tick_params(axis='x', labelrotation=45)


plt.tight_layout()
plt.show()

"""The raw counts suggested a peak in diabetes cases around BMI 28, but this was due to population concentration. After normalizing by prevalence, a clear trend appears - diabetes risk rises with BMI, exceeding 40% in the highest categories. Normalization thus reveals the true relationship between BMI and diabetes.

### Health Vs DIABETES
"""

plt.figure(figsize=(6,3))
sns.histplot(df['MentHlth'], bins=31, kde=False, color="skyblue")

plt.title("Distribution of Days of Poor Mental Health", fontsize=14, fontweight="bold")
plt.xlabel("Days of Poor Mental Health (0–30)", fontsize=12)
plt.ylabel("Number of People", fontsize=12)

plt.grid(axis='y', alpha=0.3)
sns.despine()

plt.tight_layout()
plt.show()

# Count diabetic cases by number of poor mental health days
counts_ment = (
    df.loc[df['Diabetes_binary'] == 1, 'MentHlth']
        .round()
        .value_counts()
        .sort_index()
)

plt.figure(figsize=(6,3))
sns.barplot(x=counts_ment.index, y=counts_ment.values, color="lightgreen")

plt.title("Diabetic Cases by Days of Poor Mental Health", fontsize=14, fontweight="bold")
plt.xlabel("Days of Poor Mental Health (0–30)", fontsize=12)
plt.ylabel("Number of People with Diabetes", fontsize=12)

plt.xticks(ticks=np.arange(0, 31, 2))

sns.despine()
plt.grid(axis='y', alpha=0.3)

plt.tight_layout()
plt.show()

#MentHlth (days not good in last 30) to categorical
ment_bins   = [0, 1, 14, 30, 31]
ment_labels = ['0 days', '1–13 days', '14–29 days', '30 days']
df['MentHlth_cat'] = pd.Categorical(
    pd.cut(df['MentHlth'], bins=ment_bins, labels=ment_labels, right=False),
    categories=ment_labels, ordered=True
)

# Color palette
colors = sns.color_palette("Set2", n_colors=df['MentHlth_cat'].nunique())

plt.figure(figsize=(10, 6))
ax = sns.barplot(
    x='MentHlth_cat',
    y='Diabetes_binary',
    data=df,
    palette=colors,
    errorbar=None
)

ax.set_title("Diabetes Prevalence by Mental Health Status", fontsize=14, fontweight="bold")
ax.set_xlabel("Days of Poor Mental Health (in last 30 days)", fontsize=12)
ax.set_ylabel("Prevalence of Diabetes", fontsize=12)

# Format y-axis as %
ax.yaxis.set_major_formatter(PercentFormatter(1.0))

plt.xticks(rotation=45, ha="right")

for container in ax.containers:
    ax.bar_label(container, labels=[f"{b.get_height()*100:.1f}%" for b in container], padding=2)

sns.despine()
ax.grid(axis='y', alpha=0.3)

plt.tight_layout()
plt.show()

"""Most people reported either 0 or 30 poor mental health days, and diabetics clustered in the same groups. After normalizing, a clear trend emerged - diabetes prevalence rises with worsening mental health, from about 13% at 0 days to nearly 23% at 30 days. This shows a strong link between poor mental health and higher diabetes risk."""

# PhysHlth (days not good in last 30) → categorical bins
phys_bins   = [0, 1, 14, 30, 31]
phys_labels = ['0 days', '1–13 days', '14–29 days', '30 days']

df['PhysHlth_cat'] = pd.cut(
    df['PhysHlth'],
    bins=phys_bins,
    labels=phys_labels,
    right=False,
    include_lowest=True
)

# Right: Distribution of PhysHlth for ALL people
plt.figure(figsize=(8,6))
sns.histplot(df['PhysHlth'], bins=31, kde=False, color="lightsteelblue")
plt.title("Distribution of Days of Poor Physical Health", fontsize=14, fontweight="bold")
plt.xlabel("Days of Poor Physical Health (0–30)", fontsize=12)
plt.ylabel("Number of People", fontsize=12)
plt.grid(axis='y', alpha=0.3)
sns.despine()
plt.tight_layout()
plt.show()

# Diabetic cases by days of poor physical health
counts_phys = (
    df.loc[df['Diabetes_binary'] == 1, 'PhysHlth']
      .round().value_counts().sort_index()
)

plt.figure(figsize=(8,6))
sns.barplot(x=counts_phys.index, y=counts_phys.values, color="skyblue")
plt.title("Diabetic Cases by Days of Poor Physical Health", fontsize=14, fontweight="bold")
plt.xlabel("Days of Poor Physical Health (0–30)", fontsize=12)
plt.ylabel("Number of People with Diabetes", fontsize=12)
plt.xticks(np.arange(0, 31, 2))
plt.grid(axis='y', alpha=0.3)
sns.despine()
plt.tight_layout()
plt.show()

plt.figure(figsize=(10, 6))
ax = sns.barplot(
    data=df,
    x='PhysHlth_cat', y='Diabetes_binary',
    hue='PhysHlth_cat',
    palette=sns.color_palette("Set2", n_colors=len(phys_labels)),
    dodge=False, legend=False, errorbar=None
)

ax.set_title("Diabetes Prevalence by Physical Health Status", fontsize=14, fontweight="bold")
ax.set_xlabel("Days of Poor Physical Health (last 30 days)", fontsize=12)
ax.set_ylabel("Prevalence of Diabetes", fontsize=12)
ax.yaxis.set_major_formatter(PercentFormatter(1.0))
plt.xticks(rotation=45, ha="right")

for container in ax.containers:
    ax.bar_label(container, labels=[f"{b.get_height()*100:.1f}%" for b in container], padding=2)

sns.despine()
ax.grid(axis='y', alpha=0.3)
plt.tight_layout()
plt.show()

"""We first plotted the distribution of poor physical health days to see where the population concentrates (mostly at 0 and 30) and compared it with the raw counts of diabetics by day, which showed similar clustering driven by group size. To correct for this, we normalized by computing diabetes prevalence within each PhysHlth category. The normalized chart shows a clear gradient: prevalence rises markedly with more poor-health days, increasing from about 10% at 0 days to around 30% at 30 days, providing an actionable signal for modeling.

"""

# Distribution of general health (all people)
plt.figure(figsize=(8,6))
sns.countplot(x='GenHlth', data=df, color="lightsteelblue")
plt.title("Distribution of General Health in Population", fontsize=14, fontweight="bold")
plt.xlabel("General Health (1=Excellent → 5=Poor)", fontsize=12)
plt.ylabel("Number of People", fontsize=12)
plt.grid(axis='y', alpha=0.3)
sns.despine()
plt.tight_layout()
plt.show()

# Diabetic cases by general health category
counts_gen = (
    df.loc[df['Diabetes_binary'] == 1, 'GenHlth']
      .value_counts().sort_index()
)

plt.figure(figsize=(8,6))
sns.barplot(x=counts_gen.index, y=counts_gen.values, color="skyblue")
plt.title("Diabetic Cases by General Health", fontsize=14, fontweight="bold")
plt.xlabel("General Health (1=Excellent → 5=Poor)", fontsize=12)
plt.ylabel("Number of People with Diabetes", fontsize=12)
plt.grid(axis='y', alpha=0.3)
sns.despine()
plt.tight_layout()
plt.show()

# Define bins/labels and build the categorical column
gen_bins   = [1, 2, 3, 4, 5, 6]
gen_labels = ['Excellent', 'Very good', 'Good', 'Fair', 'Poor']

df['GenHlth_cat'] = pd.Categorical(
    pd.cut(pd.to_numeric(df['GenHlth'], errors='coerce').clip(1, 5),
           bins=gen_bins, labels=gen_labels, right=False),
    categories=gen_labels, ordered=True
)

plt.figure(figsize=(8,6))
ax = sns.barplot(
    data=df,
    x='GenHlth_cat', y='Diabetes_binary',
    hue='GenHlth_cat',
    palette=sns.color_palette("Set2", n_colors=len(gen_labels)),
    dodge=False, legend=False, errorbar=None
)

ax.set_title("Diabetes Prevalence by General Health", fontsize=14, fontweight="bold")
ax.set_xlabel("General Health (Excellent → Poor)", fontsize=12)
ax.set_ylabel("Prevalence of Diabetes", fontsize=12)
ax.yaxis.set_major_formatter(PercentFormatter(1.0))

for container in ax.containers:
    ax.bar_label(container, labels=[f"{b.get_height()*100:.1f}%" for b in container], padding=2)

sns.despine()
ax.grid(axis='y', alpha=0.3)
plt.tight_layout()
plt.show()

"""Comparing the population distribution of GenHlth with the raw number of diabetics shows higher counts in larger groups, especially among those reporting "fair" or "poor" health. Because raw counts track group size, we normalized to within-category prevalence. The normalized view shows a steep gradient: prevalence is very low among respondents with excellent health and exceeds 30% among those reporting poor health, highlighting general health as a strong predictor of diabetes risk."""

# predictors-only, drop constants, and use Spearman
preds = df.select_dtypes('number').drop(columns=['Diabetes_binary'], errors='ignore')
preds = preds.loc[:, preds.nunique(dropna=True) > 1]

corr = preds.corr(method='spearman')

plt.figure(figsize=(11, 9))
sns.heatmap(corr, cmap='coolwarm', center=0)
plt.title('Correlation matrix (predictors only, Spearman)')
plt.tight_layout()
plt.show()

# keep cleaned versions; drop raw duplicates
drop_raw = ['BMI','MentHlth','PhysHlth']
preds = df.select_dtypes('number').drop(columns=['Diabetes_binary'] + drop_raw, errors='ignore')
preds = preds.loc[:, preds.nunique(dropna=True) > 1]

corr = preds.corr(method='spearman')

mask = np.triu(np.ones_like(corr, dtype=bool))   # hide upper triangle
plt.figure(figsize=(11,9))
sns.heatmap(corr, mask=mask, cmap='coolwarm', center=0, linewidths=.5)
plt.title('Correlation matrix (predictors only, Spearman)')
plt.tight_layout(); plt.show()

"""The correlation matrix highlights BMI, high blood pressure, and heart disease or stroke as the strongest predictors of diabetes. Moderate correlations appear with cholesterol, mobility issues, and mental health, while factors like smoking, alcohol, and healthcare coverage show weak links. Overall, it confirms that metabolic and heart-related factors dominate diabetes risk. Importantly, we did not rely solely on the seemingly weak correlations here to exclude variables, but later validated their contribution using exact measures.

## Data Preparation

---
"""

df.info()

to_drop = ['Fruits' ,'Veggies','Sex', 'NoDocbcCost', 'AnyHealthcare', 'BMI_rounded']

df.drop(to_drop, axis=1, inplace=True)

df.info()

def should_drop(col):
    gap  = abs(df.loc[df[col]==1,'Diabetes_binary'].mean()
               - df.loc[df[col]==0,'Diabetes_binary'].mean())
    return (gap < 0.07)

bin_cols = [c for c in df.columns
            if c != 'Diabetes_binary'
            and pd.api.types.is_numeric_dtype(df[c])
            and df[c].dropna().isin([0,1]).all()]

for c in bin_cols:
    print(c, 'DROP' if should_drop(c) else 'KEEP')

"""Rule: for each 0/1 feature, we check how much the diabetes rate differs between the “1” group and the “0” group.
If that gap is under 7 percentage points, we drop the feature as too weak.
Why 7%? It kept features with a clear effect in our charts and removed the near-flat ones, giving a simpler model without losing useful signal.
"""

df.info()

binary_columns = [col for col in df.columns
                  if df[col].nunique(dropna=True) == 2]

print("Binary columns:", binary_columns)

#converting binary columns into integer
df[binary_columns] = df[binary_columns].astype(int)

#find non numeric binary columns
# Then filter out non-numeric ones
non_numeric_binary = [col for col in binary_columns
                      if not pd.api.types.is_numeric_dtype(df[col])]

print("Non-numeric binary columns:", non_numeric_binary)

df.info()

# Step 1: Get all numeric columns
numeric_columns = [col for col in df.columns if pd.api.types.is_numeric_dtype(df[col])]
print("Numeric columns:", numeric_columns)

# Step 2: Filter out binary (only 2 unique values)
non_binary_numeric = [col for col in numeric_columns if df[col].nunique(dropna=True) > 2]

print("Numeric columns that are not binary:", non_binary_numeric)

#dropping 'Age', 'Education', 'Income',GenHlth (we have them in category)
non_binary_numeric = [col for col in non_binary_numeric if col not in ['Age', 'Education', 'Income','GenHlth']]

print("Numeric columns that are not binary after dropping Age, Education, and Income:", non_binary_numeric)

# Manual Min-Max scaling that keeps the original
normalized_cols = []          # list to track new column names

for col in non_binary_numeric:
    min_val = df[col].min()
    max_val = df[col].max()
    new_col = col + '_norm'   # name for the normalized version

    if min_val == max_val:    # avoid division by zero
        df[new_col] = 0
    else:
        df[new_col] = (df[col] - min_val) / (max_val - min_val)

    normalized_cols.append(new_col)

print("Normalized columns:", normalized_cols)

df.info()

cats_to_drop = ['BMI_cat','MentHlth_cat','PhysHlth_cat']
df.drop(cats_to_drop, axis=1, inplace=True)

df.info()

# Step 1: Identify non-numeric columns
non_numeric_columns = [col for col in df.columns
                       if not pd.api.types.is_numeric_dtype(df[col])]
print("Non-numeric columns:", non_numeric_columns)

# Step 2: One-hot encode with dtype=int to force 0/1
df = pd.get_dummies(df, columns=non_numeric_columns, drop_first=True, dtype=int)

print("One-hot encoding complete with 0/1 values. New shape:", df.shape)

df.info()

df_final = df.drop(columns = ['Age', 'Education', 'Income','GenHlth','BMI','PhysHlth','MentHlth',])
df_final.info()

"""## Modeling & Evaluation

---


"""

from sklearn.model_selection import train_test_split
import torch

#speparate the target colums from the rest
X = df_final.drop(columns=['Diabetes_binary'])
Y = df_final['Diabetes_binary']

#Split train test
X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.2,random_state=42,stratify=Y)

print("Train class ratio:\n", Y_train.value_counts(normalize=True))
print("Test  class ratio:\n", Y_test.value_counts(normalize=True))

X_test_tensor = torch.tensor(X_test.values, dtype = torch.float32)
Y_test_tensor = torch.tensor(Y_test.values, dtype = torch.float32).unsqueeze(1) ##.unsqueeze(1) - change shape from (N) to (N,1)
print(X_test_tensor.shape)
print(Y_test_tensor.shape)

X_train_tensor_reg = torch.tensor(X_train.values, dtype = torch.float32)
Y_train_tensor_reg = torch.tensor(Y_train.values, dtype = torch.float32).unsqueeze(1) ##.unsqueeze(1) - change shape from (N) to (N,1)

#check shapes
print(X_train_tensor_reg.shape)
print(Y_train_tensor_reg.shape)

import torch.nn as nn
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

"""###Pre Undersampling Model"""

class DiabetesClassifier1(nn.Module):
  def __init__(self,input_dim,hidden_dim = 64, dropout = 0.3):
    super().__init__()
    self.model = nn.Sequential(
        nn.Linear(input_dim,hidden_dim),
        nn.BatchNorm1d(hidden_dim),
        nn.ReLU(),
        nn.Dropout(dropout),
        nn.Linear(hidden_dim,hidden_dim),
        nn.BatchNorm1d(hidden_dim),
        nn.ReLU(),
        nn.Dropout(dropout),
        nn.Linear(hidden_dim,1),
        nn.Sigmoid()
    )
  def forward(self,x):
    return self.model(x)

#plotting confusions matrix nicely
def plot_confusion_matrix(cm, display_labels, title = "Confusion Matrix"):
  disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=display_labels)
  flg, ax = plt.subplots(figsize = (8,6))
  disp.plot(ax=ax, cmap="Blues", values_format='d')
  plt.title(title)
  plt.show()

#Building model object
InputDim_reg = X_train_tensor_reg.shape[1]
model_reg = DiabetesClassifier1(InputDim_reg)
criterion_reg = nn.BCELoss()
optimizer_reg = torch.optim.Adam(model_reg.parameters(), lr = 0.001)

#Hyper parametrs
epochs_reg = 20
batch_size_reg = 64
device_reg = torch.device("cuda" if torch.cuda.is_available() else "cpu")

#Define data loader
train_data_set_reg = torch.utils.data.TensorDataset(X_train_tensor_reg, Y_train_tensor_reg)
train_loader_reg = torch.utils.data.DataLoader(train_data_set_reg, batch_size= batch_size_reg, shuffle=True)

train_losses_reg = []
# Training Loop
for epoch in range(epochs_reg):
    model_reg.train() #train mode

    epoch_loss = 0
    correct = 0
    total = 0

    for X_batch, y_batch in train_loader_reg:
        X_batch = X_batch.to(device_reg)
        y_batch = y_batch.to(device_reg)

        # Forward
        # outputs are probabilities in the range 0-1
        outputs = model_reg(X_batch) # runs forward

        # Compute loss function
        loss = criterion_reg(outputs, y_batch)

        # Backward
        optimizer_reg.zero_grad()
        loss.backward()

        # Update paraneters
        optimizer_reg.step()

        epoch_loss += loss.item() * X_batch.size(0)

        # Compute accuracy

        # 0.5 is the decision boundary
        preds = (outputs >= 0.5).float()

        correct += (preds == y_batch).sum().item()
        total += y_batch.size(0)

    epoch_loss /= len(train_loader_reg.dataset)
    train_losses_reg.append(epoch_loss)
    accuracy = correct / total

    print(f"Epoch {epoch+1}/{epochs_reg} | Loss: {epoch_loss:.4f} | Accuracy: {accuracy:.4f}")

#Plot training loss curve
plt.figure(figsize=(8,5))
plt.plot(range(1, epochs_reg + 1), train_losses_reg, marker='o')
plt.title('Training Loss over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Binary-Cross-Entropy Loss')
plt.grid(True)
plt.tight_layout()
plt.show()

# Evaluation on test set
model_reg.eval()
with torch.no_grad():
    X_test_tensor = X_test_tensor.to(device_reg)
    Y_test_tensor = Y_test_tensor.to(device_reg)

    test_outputs_reg = model_reg(X_test_tensor)

    # Decision boundary 0.5
    test_preds_reg = (test_outputs_reg >= 0.5).float()
    test_accuracy_reg = (test_preds_reg == Y_test_tensor).float().mean().item()

print(f"\nTest Accuracy: {test_accuracy_reg:.4f}")

y_true_reg = Y_test_tensor.numpy()
cm_reg = confusion_matrix(y_true_reg, test_preds_reg)
class_names = ['No Diabetes', 'Diabetes']
plot_confusion_matrix(cm_reg, display_labels=class_names)

TP_reg = cm_reg[1,1]; TN_reg = cm_reg[0,0]; FN_reg = cm_reg[1,0]; FP_reg = cm_reg[0,1]
Precision_reg = TP_reg / (TP_reg + FP_reg + 1e-12)
Recall_reg    = TP_reg / (TP_reg + FN_reg + 1e-12)
print("Precision_reg:", Precision_reg)
print("Recall_reg:",    Recall_reg)

"""A simple neural network with two hidden layers of 64 units each. Each hidden layer uses batch normalization, ReLU, and dropout (p=0.3). The output layer is a single sigmoid unit. Trained with BCE loss and Adam (lr=0.001) for 20 epochs with batch size 64 (shuffled each epoch). Evaluated on the test set with a 0.5 threshold. reported accuracy and a confusion matrix.

Because the training data is imbalanced, the model mostly predicts “no diabetes” to minimize loss. This can keep accuracy looking okay but causes poor recall for the diabetes class (many false negatives in the confusion matrix). To address this class imbalance, we move to undersampling the majority class and retrain so the model pays more attention to the minority (diabetes) cases.

## Undersampling Modeling (75:25 Strategy)
"""

from imblearn.under_sampling import RandomUnderSampler

rus = RandomUnderSampler(sampling_strategy=0.3, random_state=42)
X_train_bal, Y_train_bal = rus.fit_resample(X_train, Y_train)

print("\nAfter undersampling (train):")
print(Y_train_bal.value_counts())
print(Y_train_bal.value_counts(normalize=True))

#Convert to tensors
X_train_tensor_bal = torch.tensor(X_train_bal.values, dtype = torch.float32)
Y_train_tensor_bal = torch.tensor(Y_train_bal.values, dtype = torch.float32).unsqueeze(1) ##.unsqueeze(1) - change shape from (N) to (N,1)

#check shapes
print(X_train_tensor_bal.shape)
print(Y_train_tensor_bal.shape)

"""#Model 1

"""

#Building model object
InputDim1 = X_train_tensor_bal.shape[1]
model1 = DiabetesClassifier1(InputDim1)
criterion1 = nn.BCELoss()
optimizer1 = torch.optim.Adam(model1.parameters(), lr = 0.001)

#Hyper parametrs
epochs1 = 20
batch_size1 = 64
device1 = torch.device("cuda" if torch.cuda.is_available() else "cpu")

#Define data loader
train_data_set1 = torch.utils.data.TensorDataset(X_train_tensor_bal, Y_train_tensor_bal)
train_loader1 = torch.utils.data.DataLoader(train_data_set1, batch_size= batch_size1, shuffle=True)

train_losses = []
# Training Loop
for epoch in range(epochs1):
    model1.train() #train mode

    epoch_loss = 0
    correct = 0
    total = 0

    for X_batch, y_batch in train_loader1:
        X_batch = X_batch.to(device1)
        y_batch = y_batch.to(device1)

        # Forward
        # outputs are probabilities in the range 0-1
        outputs = model1(X_batch) # runs forward

        # Compute loss function
        loss = criterion1(outputs, y_batch)

        # Backward
        optimizer1.zero_grad()
        loss.backward()

        # Update paraneters
        optimizer1.step()

        epoch_loss += loss.item() * X_batch.size(0)

        # Compute accuracy

        # 0.5 is the decision boundary
        preds = (outputs >= 0.5).float()

        correct += (preds == y_batch).sum().item()
        total += y_batch.size(0)

    epoch_loss /= len(train_loader1.dataset)
    train_losses.append(epoch_loss)
    accuracy = correct / total

    print(f"Epoch {epoch+1}/{epochs1} | Loss: {epoch_loss:.4f} | Accuracy: {accuracy:.4f}")

#Plot training loss curve
plt.figure(figsize=(8,5))
plt.plot(range(1, epochs1 + 1), train_losses, marker='o')
plt.title('Training Loss over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Binary-Cross-Entropy Loss')
plt.grid(True)
plt.tight_layout()
plt.show()

# Evaluation on test set
model1.eval()
with torch.no_grad():
    X_test_tensor = X_test_tensor.to(device1)
    Y_test_tensor = Y_test_tensor.to(device1)

    test_outputs1 = model1(X_test_tensor)

    # Decision boundary 0.4
    test_preds1 = (test_outputs1 >= 0.4).float()
    test_accuracy1 = (test_preds1 == Y_test_tensor).float().mean().item()

print(f"\nTest Accuracy: {test_accuracy1:.4f}")

#confision matrix
y_true1 = Y_test_tensor.numpy()
cm1 = confusion_matrix(y_true1, test_preds1)
cm1

class_names = ['No Diabetes', 'Diabetes']
plot_confusion_matrix(cm1, display_labels=class_names)

TP1 = cm1[1,1]
TN1 = cm1[0,0]
FN1 = cm1[1,0]
FP1 = cm1[0,1]
Precision1 = TP1/(TP1 +FP1)
Recall1 = TP1/(TP1+FN1)
print(Precision1)
print(Recall1)

"""Same architecture as the last model, two hidden layers × 64 with batch normalization, ReLU, and dropout (p=0.3), sigmoid output. trained on a different dataset (undersampled 70:30 to reduce class imbalance). No changes to layers or hyperparameters.

#Model 2
"""

class DiabetesClassifier2(nn.Module):
    def __init__(self, input_dim, hidden_dim=64):
        super().__init__()
        self.model = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1),
            nn.Sigmoid()
        )
    def forward(self, x):
        return self.model(x)

InputDim2 = X_train_tensor_bal.shape[1]
model2 = DiabetesClassifier2(InputDim2)
criterion2 = nn.BCELoss()
optimizer2 = torch.optim.Adam(model2.parameters(), lr=0.001)

epochs2 = 20
batch_size2 = 64
device2 = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model2 = model2.to(device2)

train_dataset2 = torch.utils.data.TensorDataset(X_train_tensor_bal, Y_train_tensor_bal)
train_loader2  = torch.utils.data.DataLoader(train_dataset2, batch_size=batch_size2, shuffle=True)

train_losses2 = []
for ep in range(epochs2):
    model2.train()
    epoch_loss = 0.0
    correct, total = 0, 0

    for Xb, yb in train_loader2:
        Xb = Xb.to(device2)
        yb = yb.to(device2)

        optimizer2.zero_grad()
        out = model2(Xb)
        loss = criterion2(out, yb)
        loss.backward()
        optimizer2.step()

        epoch_loss += loss.item() * Xb.size(0)

        preds = (out >= 0.5).float()
        correct += (preds == yb).sum().item()
        total   += yb.size(0)

    epoch_loss /= len(train_loader2.dataset)
    acc = correct / total
    train_losses2.append(epoch_loss)
    print(f"Epoch {ep+1:02d}/{epochs2} | loss={epoch_loss:.4f} | acc={acc:.4f}")

model2.eval()
with torch.no_grad():
    Xtt = X_test_tensor.to(device2)
    Ytt = Y_test_tensor.to(device2)
    prob2 = model2(Xtt)

thr2 = 0.4
pred2 = (prob2 >= thr2).float()
test_acc2 = (pred2 == Ytt).float().mean().item()
print(f"\n[Model 2] Test Accuracy @thr={thr2}: {test_acc2:.4f}")

cm2 = confusion_matrix(Ytt.cpu().numpy(), pred2.cpu().numpy())
class_names = ['No Diabetes', 'Diabetes']
plot_confusion_matrix(cm2, display_labels=class_names)

TP2 = cm2[1,1]; TN2 = cm2[0,0]; FN2 = cm2[1,0]; FP2 = cm2[0,1]
Precision2 = TP2 / (TP2 + FP2 + 1e-12)
Recall2    = TP2 / (TP2 + FN2 + 1e-12)
print("Precision2:", Precision2)
print("Recall2:",    Recall2)

"""Same simple neural network as before: two hidden layers × 64 with batch normalization and ReLU, single sigmoid output, but without dropout. All other settings (BCE + Adam at lr=0.001, 20 epochs, batch size 64) remain the same.

#Model 3
"""

import torch, torch.nn as nn
from sklearn.metrics import confusion_matrix, f1_score, classification_report
from itertools import product
import numpy as np

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
threshold = 0.4

class Generic_Model(nn.Module):
    def __init__(self, input_dim, hidden=(96,64), p=0.3, use_bn=True):
        super().__init__()
        layers = []
        in_dim = input_dim
        for h in hidden:
            layers += [nn.Linear(in_dim, h)]
            if use_bn: layers += [nn.BatchNorm1d(h)]
            layers += [nn.ReLU(), nn.Dropout(p)]
            in_dim = h
        layers += [nn.Linear(in_dim, 1), nn.Sigmoid()]
        self.net = nn.Sequential(*layers)
    def forward(self, x): return self.net(x)

def make_optimizer(params, name, lr=1e-3):
    return torch.optim.Adam(params, lr=lr) if name=="adam" else torch.optim.SGD(params, lr=lr, momentum=0.9, nesterov=True)

def make_loss(name):
    return nn.BCELoss() if name=="bce" else nn.MSELoss()

#8-run mini experiment
def train_eval(loss_name, opt_name, batch_size, epochs=15):
    model = Generic_Model(X_train_tensor_bal.shape[1]).to(device)
    opt   = make_optimizer(model.parameters(), opt_name, lr=1e-3)
    crit  = make_loss(loss_name)

    train_ds  = torch.utils.data.TensorDataset(X_train_tensor_bal, Y_train_tensor_bal)
    train_dl  = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True)

    model.train()
    for _ in range(epochs):
        for xb, yb in train_dl:
            xb, yb = xb.to(device), yb.to(device)
            opt.zero_grad()
            out = model(xb)
            loss = crit(out, yb)
            loss.backward()
            opt.step()

    model.eval()
    with torch.no_grad():
        prob = model(X_test_tensor.to(device)).cpu().numpy().ravel()
        pred = (prob >= threshold).astype(int)
        yt   = Y_test_tensor.cpu().numpy().ravel()
        cm   = confusion_matrix(yt, pred)
        TP, TN, FN, FP = cm[1,1], cm[0,0], cm[1,0], cm[0,1]
        prec = TP/(TP+FP+1e-12); rec = TP/(TP+FN+1e-12)
        acc  = (TP+TN)/cm.sum(); f1 = f1_score(yt, pred)
    return {"loss":loss_name, "opt":opt_name, "bs":batch_size,-
            "acc":acc, "precision":prec, "recall":rec, "f1":f1, "cm":cm}

results = []
for loss_name, opt_name, bs in product(["bce","mse"], ["adam","sgd"], [32, 64]):
    metrics = train_eval(loss_name, opt_name, bs)
    results.append(metrics)
    print(f"{loss_name.upper()} | {opt_name.upper()} | bs={bs}  "
          f"-> Acc={metrics['acc']:.3f} P={metrics['precision']:.3f} "
          f"R={metrics['recall']:.3f} F1={metrics['f1']:.3f}")

"""A simple neural network with two hidden layers [96, 64], each hidden layer uses batch normalization (enabled), ReLU, and dropout (p=0.3), sigmoid output. We trained eight variants on the same 70:30 undersampled training set, combining loss {BCE, MSE} × optimizer {Adam, SGD} × batch size {32, 64}, for 15 epochs each. Evaluation used a 0.4 threshold to compute accuracy, precision, recall, F1, and a confusion matrix.

#Model 4
"""

class Chosen_Model(nn.Module):
    def __init__(self, input_dim, hidden=(96, 64), p_drop=0.3, use_bn=True):
        super().__init__()
        layers, in_dim = [], input_dim
        for h in hidden:
            layers += [nn.Linear(in_dim, h)]
            if use_bn: layers += [nn.BatchNorm1d(h)]
            layers += [nn.ReLU(), nn.Dropout(p_drop)]
            in_dim = h
        layers += [nn.Linear(in_dim, 1), nn.Sigmoid()]
        self.net = nn.Sequential(*layers)
    def forward(self, x): return self.net(x)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
input_dim = X_train_tensor_bal.shape[1]
model_prod = Chosen_Model(input_dim, hidden=(96,64), p_drop=0.3, use_bn=True).to(device)
criterion = nn.BCELoss()
optimizer = torch.optim.Adam(model_prod.parameters(), lr=1e-3)

# Parameters
batch_size = 32
epochs = 30
train_ds = torch.utils.data.TensorDataset(X_train_tensor_bal, Y_train_tensor_bal)
train_dl = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True)

# Final Production Model: BCE + Adam + bs=32
for ep in range(1, epochs+1):
    model_prod.train()
    running = 0.0
    for xb, yb in train_dl:
        xb, yb = xb.to(device), yb.to(device)
        optimizer.zero_grad()
        out = model_prod(xb)
        loss = criterion(out, yb)
        loss.backward()
        optimizer.step()
        running += loss.item() * xb.size(0)
    print(f"Epoch {ep:02d}/{epochs} | train loss={(running/len(train_dl.dataset)):.4f}")

model_prod.eval()
with torch.no_grad():
    prob = model_prod(X_test_tensor.to(device)).cpu().numpy().ravel()
    y_true = Y_test_tensor.cpu().numpy().ravel()

threshold = 0.35
y_pred = (prob >= threshold).astype(int)

cm = confusion_matrix(y_true, y_pred)
print("\nConfusion matrix:\n", cm)
print(classification_report(y_true, y_pred, digits=3))
plot_confusion_matrix(cm, display_labels=['No Diabetes', 'Diabetes'])

"""A simple neural network with two hidden layers [96, 64], each hidden layer uses batch normalization, ReLU, and dropout (p=0.3), sigmoid output. Trained with BCE loss and Adam (lr=0.001) for 30 epochs with batch size 32 on the 70:30 undersampled training set. Evaluated on the test set with a 0.35 threshold, reported accuracy, precision, recall, F1, and a confusion matrix.

#New Undersampling - (50:50 Strategy)
"""

rus50 = RandomUnderSampler(random_state=42)
X_train_bal50, Y_train_bal50 = rus50.fit_resample(X_train, Y_train)

print("\nAfter undersampling (train):")
print(Y_train_bal50.value_counts())
print(Y_train_bal50.value_counts(normalize=True))

X_train_tensor50 = torch.tensor(X_train_bal50.values, dtype = torch.float32)
Y_train_tensor50 = torch.tensor(Y_train_bal50.values, dtype = torch.float32).unsqueeze(1) ##.unsqueeze(1) - change shape from (N) to (N,1)

#check shapes
print(X_train_tensor50.shape)
print(Y_train_tensor50.shape)

"""#Model 5"""

class Chosen_Model50(nn.Module):
    def __init__(self, input_dim2, hidden2=(96, 64), p_drop2=0.3, use_bn2=True):
        super().__init__()
        layers2, in2 = [], input_dim2
        for h2 in hidden2:
            layers2 += [nn.Linear(in2, h2)]
            if use_bn2: layers2 += [nn.BatchNorm1d(h2)]
            layers2 += [nn.ReLU(), nn.Dropout(p_drop2)]
            in2 = h2
        layers2 += [nn.Linear(in2, 1), nn.Sigmoid()]
        self.net2 = nn.Sequential(*layers2)
    def forward(self, x2):
        return self.net2(x2)

device_50 = torch.device("cuda" if torch.cuda.is_available() else "cpu")
input_dim_50 = X_train_tensor50.shape[1]
model_prod2 = Chosen_Model50(input_dim_50, hidden2=(96,64), p_drop2=0.3, use_bn2=True).to(device_50)
criterion2 = nn.BCELoss()
optimizer2 = torch.optim.Adam(model_prod2.parameters(), lr=1e-3)

batch_size_50 = 32
epochs_50 = 30

train_ds_50 = torch.utils.data.TensorDataset(X_train_tensor50, Y_train_tensor50)
train_dl_50 = torch.utils.data.DataLoader(train_ds_50, batch_size=batch_size_50, shuffle=True)

for ep2 in range(1, epochs_50 + 1):
    model_prod2.train()
    run_loss2 = 0.0
    for xb2, yb2 in train_dl_50:
        xb2, yb2 = xb2.to(device_50), yb2.to(device_50)
        optimizer2.zero_grad()
        out2 = model_prod2(xb2)
        loss2 = criterion2(out2, yb2)
        loss2.backward()
        optimizer2.step()
        run_loss2 += loss2.item() * xb2.size(0)
    print(f"Epoch {ep2:02d}/{epochs_50} | train loss={(run_loss2/len(train_dl_50.dataset)):.4f}")

model_prod2.eval()
with torch.no_grad():
    probs2  = model_prod2(X_test_tensor.to(device_50)).cpu().numpy().ravel()
    y_true2 = Y_test_tensor.cpu().numpy().ravel()

threshold2 = 0.5
y_pred2 = (probs2 >= threshold2).astype(int)

cm2 = confusion_matrix(y_true2, y_pred2)
print("\nConfusion matrix (v2):\n", cm2)
print(classification_report(y_true2, y_pred2, digits=3))

plot_confusion_matrix(cm2, display_labels=['No Diabetes', 'Diabetes'], title='Confusion Matrix (Model v2)')

"""Same architecture and hyperparameters as the Chosen_Model, trained on a 50:50 undersampled training set

#New Undersamplin - (60:40 Strategy)
"""

rus60 = RandomUnderSampler(sampling_strategy=0.65,random_state=42)
X_train_bal60, Y_train_bal60 = rus60.fit_resample(X_train, Y_train)

print("\nAfter undersampling (train):")
print(Y_train_bal60.value_counts())
print(Y_train_bal60.value_counts(normalize=True))

X_train_tensor60 = torch.tensor(X_train_bal60.values, dtype = torch.float32)
Y_train_tensor60 = torch.tensor(Y_train_bal60.values, dtype = torch.float32).unsqueeze(1) ##.unsqueeze(1) - change shape from (N) to (N,1)

#check shapes
print(X_train_tensor60.shape)
print(Y_train_tensor60.shape)

"""# Model 6"""

class Chosen_Model60(nn.Module):
    def __init__(self, input_dim_60, hidden_60=(96, 64), p_drop_60=0.3, use_bn_60=True):
        super().__init__()
        layers_60, in_dim_60 = [], input_dim_60
        for h_60 in hidden_60:
            layers_60 += [nn.Linear(in_dim_60, h_60)]
            if use_bn_60: layers_60 += [nn.BatchNorm1d(h_60)]
            layers_60 += [nn.ReLU(), nn.Dropout(p_drop_60)]
            in_dim_60 = h_60
        layers_60 += [nn.Linear(in_dim_60, 1), nn.Sigmoid()]
        self.net_60 = nn.Sequential(*layers_60)
    def forward(self, x_60):
        return self.net_60(x_60)

# Device / init
device_60 = torch.device("cuda" if torch.cuda.is_available() else "cpu")
input_dim_60 = X_train_tensor60.shape[1]
model_prod60 = Chosen_Model60(input_dim_60, hidden_60=(96,64), p_drop_60=0.3, use_bn_60=True).to(device_60)

# Loss & optimizer
criterion60 = nn.BCELoss()
optimizer60 = torch.optim.Adam(model_prod60.parameters(), lr=1e-3)

batch_size_60 = 32
epochs_60 = 30

train_ds_60 = torch.utils.data.TensorDataset(X_train_tensor60, Y_train_tensor60)
train_dl_60 = torch.utils.data.DataLoader(train_ds_60, batch_size=batch_size_60, shuffle=True)

for ep_60 in range(1, epochs_60 + 1):
    model_prod60.train()
    running_60 = 0.0
    for xb_60, yb_60 in train_dl_60:
        xb_60, yb_60 = xb_60.to(device_60), yb_60.to(device_60)
        optimizer60.zero_grad()
        out_60 = model_prod60(xb_60)
        loss_60 = criterion60(out_60, yb_60)
        loss_60.backward()
        optimizer60.step()
        running_60 += loss_60.item() * xb_60.size(0)
    print(f"Epoch {ep_60:02d}/{epochs_60} | train loss={(running_60/len(train_dl_60.dataset)):.4f}")

model_prod60.eval()
with torch.no_grad():
    probs_60  = model_prod60(X_test_tensor.to(device_60)).cpu().numpy().ravel()
    y_true_60 = Y_test_tensor.cpu().numpy().ravel()

threshold_60 = 0.35
y_pred_60 = (probs_60 >= threshold_60).astype(int)

cm_60 = confusion_matrix(y_true_60, y_pred_60)
print("\nConfusion matrix (v60):\n", cm_60)
print(classification_report(y_true_60, y_pred_60, digits=3))
print(f"Accuracy (v60): {(cm_60[0,0] + cm_60[1,1]) / cm_60.sum():.3f}")

plot_confusion_matrix(cm_60, display_labels=['No Diabetes', 'Diabetes'], title='Confusion Matrix (Model v60)')

"""#### Same model different threshold"""

model_prod60.eval()
with torch.no_grad():
    probs_60  = model_prod60(X_test_tensor.to(device_60)).cpu().numpy().ravel()
    y_true_60 = Y_test_tensor.cpu().numpy().ravel()

threshold_60 = 0.5
y_pred_60 = (probs_60 >= threshold_60).astype(int)

cm_60 = confusion_matrix(y_true_60, y_pred_60)
print("\nConfusion matrix (v60):\n", cm_60)
print(classification_report(y_true_60, y_pred_60, digits=3))
print(f"Accuracy (v60): {(cm_60[0,0] + cm_60[1,1]) / cm_60.sum():.3f}")

plot_confusion_matrix(cm_60, display_labels=['No Diabetes', 'Diabetes'], title='Confusion Matrix (Model v60)')

"""Same architecture and hyperparameters as the Chosen_Model, trained on a 60:40 undersampled training set. Evaluated at two thresholds (0.35 and 0.5) to compare the precision–recall trade-off. reported accuracy, precision, recall, F1, and confusion matrices.

#Economic Model
"""

params = {
    "C_screen":   1.0,      # cost to score each person
    "C_confirm":  30.0,    # confirmatory test for predicted positives
    "C_treat":    200.0,    # early-treatment onboarding cost (TP)
    "B_TP":       6000.0,  # avoided downstream cost per TP
    "C_FN":       12000.0,  # downstream cost per missed diabetic (FN)
    "C_FP_extra": 20.0      # extra burden per FP beyond confirm test
}
SCALE  = 1e6
YLABEL = "Millions [$]"
DEC    = 1

def counts_from_sklearn_cm(cm_2x2):
    tn, fp, fn, tp = int(cm_2x2[0,0]), int(cm_2x2[0,1]), int(cm_2x2[1,0]), int(cm_2x2[1,1])
    return tn, fp, fn, tp

def evaluate_counts(TN, FP, FN, TP, params, name="Model"):
    N = TN + FP + FN + TP
    screening_cost = N  * params["C_screen"]
    confirm_cost   = (TP + FP) * params["C_confirm"]
    treat_cost     = TP * params["C_treat"]
    fp_extra_cost  = FP * params["C_FP_extra"]
    missed_cost    = FN * params["C_FN"]
    total_costs    = screening_cost + confirm_cost + treat_cost + fp_extra_cost + missed_cost
    total_benefits = TP * params["B_TP"]
    net_total      = total_benefits - total_costs
    net_per_person = net_total / N
    return {
        "Model": name, "N": N,
        "TP": TP, "FP": FP, "TN": TN, "FN": FN,
        "Cost: screening": screening_cost,
        "Cost: confirm": confirm_cost,
        "Cost: treat (TP)": treat_cost,
        "Cost: FP extra": fp_extra_cost,
        "Cost: missed (FN)": missed_cost,
        "Total Costs": total_costs,
        "Total Benefits": total_benefits,
        "Net Value (total)": net_total,
        "Net Value (per person)": net_per_person
    }

def evaluate_from_sklearn_cm(cm, params, name="Model"):
    TN, FP, FN, TP = counts_from_sklearn_cm(cm)
    return evaluate_counts(TN, FP, FN, TP, params, name=name)

def plot_cost_components(res):
    labels = ["Cost: screening","Cost: confirm","Cost: treat (TP)","Cost: FP extra","Cost: missed (FN)"]
    values = [res[k] for k in labels]
    vals   = [v / SCALE for v in values]

    plt.figure()
    bars = plt.bar(labels, vals)
    plt.title(f"Cost Components — {res['Model']}")
    plt.ylabel(YLABEL)
    plt.xticks(rotation=15)
    plt.gca().yaxis.grid(True, linestyle="--", alpha=0.4)
    for i, b in enumerate(bars):
        plt.text(b.get_x()+b.get_width()/2, b.get_height(),
                 f"{vals[i]:.{DEC}f}", ha="center", va="bottom", fontsize=9)
    plt.tight_layout()
    plt.show()

def plot_net(res):
    labels = ["Total Costs","Total Benefits","Net Value (total)"]
    values = [res["Total Costs"], res["Total Benefits"], res["Net Value (total)"]]
    vals   = [v / SCALE for v in values]

    plt.figure()
    bars = plt.bar(labels, vals)
    plt.title(f"Costs vs Benefits vs Net — {res['Model']}")
    plt.ylabel(YLABEL)
    plt.xticks(rotation=10)
    plt.gca().yaxis.grid(True, linestyle="--", alpha=0.4)
    for i, b in enumerate(bars):
        plt.text(b.get_x()+b.get_width()/2, b.get_height(),
                 f"{vals[i]:.{DEC}f}", ha="center", va="bottom", fontsize=9)
    plt.tight_layout()
    plt.show()

res_final = evaluate_from_sklearn_cm(cm,params, name="Model 4 (Chosen Model)")
res_u50 = evaluate_from_sklearn_cm(cm2,params, name="Model 5 (Chosen Model50)")
res_u60 = evaluate_from_sklearn_cm(cm_60,params, name="Model 6 (Chosen Model60)")

plot_cost_components(res_final)
plot_net(res_final)

plot_cost_components(res_u50)
plot_net(res_u50)

plot_cost_components(res_u60)
plot_net(res_u60)

df_compare = pd.DataFrame([res_final, res_u50, res_u60])
print(df_compare[["Model","N","TP","FP","TN","FN","Total Costs","Total Benefits","Net Value (total)","Net Value (per person)"]])

from matplotlib.ticker import FuncFormatter

# Example: adjust for your actual variables
vals   = (df_compare["Net Value (total)"].to_numpy(dtype=float)) / SCALE
labels = df_compare["Model"].astype(str).tolist()

# Sort bars for readability (highest to lowest)
order  = np.argsort(-vals)
vals   = vals[order]
labels = [labels[i] for i in order]

# Colors by sign
colors = np.where(vals >= 0, "#2E7D32", "#C62828")  # green for +, red for -

fig, ax = plt.subplots(figsize=(10, 6))
bars = ax.bar(labels, vals, color=colors, edgecolor="#1b1b1b", linewidth=0.6)

# Title & axis labels
ax.set_title("Economic Comparison — Net Value (total)", fontsize=14, weight="bold")
ax.set_ylabel("Millions [$]", fontsize=12)

# Y-axis formatting with DEC precision (default: 2 if not set)
DEC = 2
ax.yaxis.set_major_formatter(FuncFormatter(lambda y, _: f"{y:,.{DEC}f}"))

# Style
ax.grid(True, axis="y", linestyle="--", alpha=0.35)
ax.set_axisbelow(True)
ax.spines["top"].set_visible(False)
ax.spines["right"].set_visible(False)
plt.xticks(rotation=15)

# Zero line
ax.axhline(0, color="#444444", linewidth=1)

# Value labels
offset = max(abs(vals)) * 0.03 if vals.size else 0.02
for b, v in zip(bars, vals):
    y = b.get_height()
    dy = offset if y >= 0 else -offset
    ax.text(
        b.get_x() + b.get_width()/2, y + dy,
        f"{v:,.{DEC}f}",
        ha="center", va=("bottom" if y >= 0 else "top"), fontsize=10
    )

ax.margins(y=0.15)
fig.tight_layout()
plt.show()